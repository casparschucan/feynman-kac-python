{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108d37ce-58e9-430b-8c05-0df3060a575a",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be813589-3831-4e2d-be7e-9aedd51890ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mlmc import mlmc  # Assuming this is how you import the function\n",
    "\n",
    "def benchmark_mlmc(parameter_sets, runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark the mlmc function with different parameter sets.\n",
    "\n",
    "    Args:\n",
    "        parameter_sets: List of dictionaries, each containing parameters for mlmc\n",
    "        runs: Number of times to run each parameter set for averaging\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with benchmark results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for params in parameter_sets:\n",
    "        total_time = 0\n",
    "        total_cost = 0\n",
    "        max_levels = []\n",
    "\n",
    "        for _ in range(runs):\n",
    "            start_time = time.time()\n",
    "            expectation, cost, max_level, _ = mlmc(**params)\n",
    "            end_time = time.time()\n",
    "\n",
    "            total_time += (end_time - start_time)\n",
    "            total_cost += cost\n",
    "            max_levels.append(max_level)\n",
    "\n",
    "        results.append({\n",
    "            **params,\n",
    "            'avg_execution_time': total_time / runs,\n",
    "            'avg_computational_cost': total_cost / runs,\n",
    "            'avg_max_level': sum(max_levels) / len(max_levels),\n",
    "            'expectation': expectation  # Last computed value\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaa1c1-4cfa-4e51-9eb7-5526bf045bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test functions from your notebook\n",
    "from test_functions import non_hom_test, test_phi, test_bound, test_rhs, test_cos, test_cos_rhs\n",
    "\n",
    "# Define parameter sets to benchmark\n",
    "parameter_sets = [\n",
    "    # Varying epsilon (accuracy)\n",
    "    {'x': 0.5, 'y': 0.5, 'f': test_bound, 'g': test_rhs, 'dt0': 0.01, 'epsilon': 0.1},\n",
    "    {'x': 0.5, 'y': 0.5, 'f': test_bound, 'g': test_rhs, 'dt0': 0.01, 'epsilon': 0.025},\n",
    "    {'x': 0.5, 'y': 0.5, 'f': test_bound, 'g': test_rhs, 'dt0': 0.01, 'epsilon': 0.1/(4**2)},\n",
    "\n",
    "    # Varying dt0 (initial time step)\n",
    "    {'x': 0.5, 'y': 0.5, 'f': test_bound, 'g': test_rhs, 'dt0': 0.02, 'epsilon': 0.01},\n",
    "    {'x': 0.5, 'y': 0.5, 'f': test_bound, 'g': test_rhs, 'dt0': 0.005, 'epsilon': 0.01},\n",
    "\n",
    "    # Different test functions\n",
    "    {'x': 0.5, 'y': 0.5, 'f': test_cos, 'g': test_cos_rhs, 'dt0': 0.01, 'epsilon': 0.01},\n",
    "\n",
    "    # Different coordinates\n",
    "    {'x': 0.25, 'y': 0.75, 'f': test_bound, 'g': test_rhs, 'dt0': 0.01, 'epsilon': 0.01},\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = benchmark_mlmc(parameter_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6bac52-2531-4ce9-b2dc-d242c1ee14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "print(benchmark_results)\n",
    "\n",
    "# Plot execution time vs epsilon\n",
    "plt.figure(figsize=(10, 6))\n",
    "epsilon_results = benchmark_results[benchmark_results['x'] == 0.5]\n",
    "epsilon_results = epsilon_results[epsilon_results['dt0'] == 0.1]\n",
    "epsilon_results = epsilon_results[epsilon_results['f'] == test_bound]\n",
    "epsilon_results = epsilon_results.sort_values('epsilon')\n",
    "\n",
    "plt.loglog(epsilon_results['epsilon'], epsilon_results['avg_execution_time'], 'o-', label='Execution Time')\n",
    "plt.loglog(epsilon_results['epsilon'], epsilon_results['avg_computational_cost'], 's-', label='Computational Cost')\n",
    "plt.xlabel('Epsilon (tolerance)')\n",
    "plt.ylabel('Time/Cost (log scale)')\n",
    "plt.title('MLMC Performance vs Tolerance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot max level vs epsilon\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(epsilon_results['epsilon'], epsilon_results['avg_max_level'], 'o-')\n",
    "plt.xlabel('Epsilon (tolerance)')\n",
    "plt.ylabel('Average Max Level')\n",
    "plt.title('MLMC Max Level vs Tolerance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
